
+ Numerical Problems on: 

	Hebb,
	MP neuron,
	Perceptron,
	Backpropogation,
	Logistic regression,
	Principal Component Analysis,
	Linear regression,
	diagonalization


1. Machine learning: 
is a subfield of artificial intelligence (AI) that focuses on developing 
algorithms and techniques that enable computers to learn from and make predictions 
or decisions based on data. Instead of being explicitly programmed to perform a certain 
task, machine learning systems are designed to learn and improve their performance 
over time as they are exposed to more data.


2. Issues in ML: 
	+ Inadequate Training Data: 
		Insufficient or poor-quality data can hinder machine learning algorithms, leading to inaccurate predictions and decisions. 
	+ Overfitting and Underfitting
	+ Non-representative Training Data: 
		If the training data doesnt accurately represent the real-world cases, the model's predictions may be less accurate and biased. 
	+ Poor Data Quality: 
		Noisy, incomplete, or inaccurate data can lead to low-quality results and affect the performance of machine learning models. 
	+ Data Drift / Updating: 
		Changes in data over time can lead to outdated or incorrect recommendations from machine learning models, requiring regular 
		updates and adjustments. 
	+ Lack of Skilled Resources: 
		The shortage of skilled professionals with expertise in mathematics, science, and technology poses a challenge in developing and 
		managing machine learning solutions. 
	+ Process Complexity: 
		The machine learning process involves various complex tasks such as data analysis, model training, and evaluation, making it 
		challenging and time-consuming. 
	+ Slow Implementation: 
		Training large models can be time-consuming. Optimizing code, using efficient libraries, and parallelizing computations are 
		essential.
	+ Imperfections in Algorithms as Data Grows: 
		As data scales, algorithms may exhibit unexpected behavior. Regular updates and monitoring are necessary to maintain model 
		accuracy.
	

3. Applications of ML:
	Natural Language Processing (NLP): ML powers chatbots, language translation, sentiment analysis, and more.
	Computer Vision: ML helps in image recognition, object detection, and facial recognition.
	Recommendation Systems: Think of personalized recommendations on streaming platforms or e-commerce sites.
	Healthcare: ML aids in disease diagnosis, drug discovery, and patient monitoring.
	Finance: Fraud detection, stock market prediction, and credit scoring benefit from ML.


4.  Steps for developing a ml application:
	+ Define the Problem:
		Clearly articulate the issue or task that the machine learning application will address. 
	+ Collect and Prepare Data:
		Gather relevant data and clean, format, and preprocess it for analysis. 
	+ Data Exploration and Analysis:
		Investigate the data to understand its characteristics and identify patterns. 
	+ Feature Engineering:
		Create or select informative features from the data to improve model performance. 
	+ Select a Model:
		Choose an appropriate machine learning algorithm suited to the problem and data. 
	+ Train the Model:
		Use the training data to teach the model to make predictions or classifications. 
	+ Evaluate the Model:
		Assess the model's performance using testing data to ensure it generalizes well. 
	+ Iterate and Improve:
		Refine the model by adjusting parameters and features based on evaluation results. 
	+ Deploy the Model:
		Put the trained model into operation within the desired application or system. 
	+  Monitor and Maintain:
		Continuously assess model performance and update as needed to ensure continued effectiveness. 
	+  Scale and Optimize:
		Expand the application's capabilities to handle increased data volume and optimize performance. 
	+  Ensure Security and Compliance:
		Implement measures to protect data privacy and comply with relevant regulations and standards. 


5. Training and testing are essential steps in the development of machine learning models:
During training, the model learns patterns and relationships in the data, while testing 
evaluates the model's performance on unseen data to assess its ability to generalize. 
Here's an example to illustrate these concepts: 
Let's consider a machine learning task of classifying images of fruits into categories 
such as apples, oranges, and bananas. 

	Training:	
		+ Data Collection: 
			Gather a dataset of labeled images of fruits, where each image is labeled with the type of fruit it represents. 
		+ Training Phase: 
			Split the dataset into two parts: a training set and a testing set. For example, 70% of the data might be allocated for 
			training and 30% for testing. 
		+ Model Training: 
			Use the training set to train a machine learning model, such as a convolutional neural network (CNN). During training, the 
			model learns to recognize patterns and features in the images that are indicative of each fruit type. 
		+ Iterative Process: 
			Train the model iteratively by adjusting its parameters until it achieves satisfactory performance. 
	Testing:
		+ Model Evaluation: 
			After training, evaluate the trained model's performance using the testing set, which contains images that the model 
			hasn't seen during training. 
		+ Prediction: 
			Present each image in the testing set to the model and have it predict the fruit type. 
		+ Comparison: 
			Compare the model's predictions to the actual labels of the images in the testing set. 
		+ Metrics: 
			Calculate evaluation metrics such as accuracy, precision, recall, and F1-score to quantify the model's performance. 

For example, suppose the trained model correctly classifies 80% of the images in the 
testing set. This means that it accurately identifies the type of fruit in 80% of the unseen 
images. If the model's performance is satisfactory based on the chosen evaluation metrics, it can be deployed for real-world use. 


6. Cross validation:
Cross-validation is a technique used to assess the performance and generalization ability of a machine learning model.
is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves splitting the dataset into 
multiple subsets, called folds, and iteratively training and testing the model on different combinations of these folds. Here's how 
cross-validation works: 
	+ Data Splitting: 
		The dataset is divided into k subsets of approximately equal size, typically referred to as folds. For example, in 5-fold 
		cross-validation, the data is divided into 5 equal parts.
	+ Model Training and Testing: 
		The model is trained k times, with each iteration using a different fold as the testing set and the remaining folds as the 
		training set. For instance, in the first iteration, the first fold is used as the testing set, and the model is trained on the 
		remaining four folds. In the second iteration, the second fold is used as the testing set, and so on. 
	+ Performance Evaluation: 
		After each iteration, the model's performance is evaluated using the testing set (the fold that was not used for training). 
		Evaluation metrics such as accuracy, precision, recall, and F1-score are calculated. 
	+ Average Performance: 
		The performance metrics obtained from each iteration are averaged to obtain a more robust estimate of the model's performance.
Advantages:
	+ Better Performance Estimation
	+ Reduced Overfitting
	+ Optimal Hyperparameter Tuning:
		By using cross-validation, the hyperparameters can be tuned based on their performance across multiple folds, leading to better 
		generalization.
	+ Utilization of Data: 
		Cross-validation allows for maximum utilization of the available data for both training and testing purposes.


7.  ML Definitions:

- Training error: 
	(also known as residual error) measures how well a model fits the training data. A low training error indicates that the model 	  
	closely approximates the training data, but it doesn’t necessarily guarantee good performance on unseen data.

- Generalization error: 
	(also called test error) measures how well a model performs on new, unseen data. It reflects the model’s ability to 		  
	generalize patterns learned from the training data to previously unseen examples. A model with low generalization error is robust 
	and performs well on both training and test data.

- Overfitting: 
	is the scenario in which the model becomes complex and learns too much on the training data and fails genralize patterns in the 
	new unseen data. To overcome overfitting, techniques like regularization, cross-validation, and reducing model complexity are 
	used.

- Underfitting: 
	happens when a model is too simple to capture the underlying patterns in the data. It results in poor performance on both 
	training and test data. 

- The bias-variance trade-off: 
is a delicate balance between two types of errors:

	- Bias: 
		The difference between the model’s predictions and the true values (high bias leads to underfitting).
		A model with high bias tends to make simplistic assumptions about the data and may underfit the training data.
	- Variance: 
		The variability of model predictions for different training datasets (high variance leads to overfitting).
		A model with high variance is sensitive to small fluctuations in the training data and may overfit the training data.

An ideal model strikes a balance between bias and variance. Increasing model complexity reduces bias but increases variance, and 
vice versa. The goal is to find the sweet spot where both bias and variance are minimized, leading to optimal generalization.

+ Here's how bias and variance contribute to overfitting and underfitting: 

	• Underfitting: Models with high bias and low variance are prone to underfitting. 
		They are too simple to capture the underlying patterns in the data, leading to 
		poor performance on both the training and testing data. 
	• Overfitting: Models with low bias and high variance are prone to overfitting. They 
		are too complex and flexible, capturing noise and irrelevant details in the training 
		data. As a result, they perform well on the training data but poorly on the testing 
		data due to their inability to generalize.


8. Clustering:  
is a method of grouping the objects into clusters such that objects with most similarities remains into a group and has less or no 
similarities with the objects of another group. Clustering is an unsupervised learning method that groups data points based on their 
similarities. Unlike classification, clustering does not rely on predefined labels or class information.


9. Confusion matrix: 
<Give example if required: classifying a image into dog or cat>
is essential for assessing a classification model’s performance. summarizes the performance of a machine learning model on a set of test 
data. It is a means of displaying the number of accurate and inaccurate instances based on the model’s predictions. It provides a detailed
analysis of the following predictions:
	True Positives (TP): Correctly predicted positive instances.
	True Negatives (TN): Correctly predicted negative instances.
	False Positives (FP): Incorrectly predicted positive instances.
	False Negatives (FN): Incorrectly predicted negative instances.
The confusion matrix helps us understand recall, accuracy, precision, and overall effectiveness in class distinction.

<Terms:>

	+ Accuracy
		is used to measure the performance of the model. It is the ratio of Total correct instances to the total instances. 
			Accuracy = TP + TN / TP + TN + FP + FN 
	+ Precision:
		is a measure of how accurate a model’s positive predictions are. It is defined as the ratio of true positive predictions to the 
		total number of positive predictions made by the model.
			P = TP / TP + FP 
	+ Recall:
		measures the effectiveness of a classification model in identifying all relevant instances from a dataset. It is the ratio of the 
		number of true positive (TP) instances to the sum of true positive and false negative (FN) instances.
			Recall = TP / TP + FN
	+ Specificity:
		is another important metric in the evaluation of classification models, particularly in binary classification. It measures the 
		ability of a model to correctly identify negative instances. Specificity is also known as the True Negative Rate.
			S = TN / TN + FP
	+ F1-score:
		is used to evaluate the overall performance of a classification model. It is the harmonic mean of precision and recall
			f1-score = 2.Precision.Recall / Precision + Recall.


// ----------------------------------------------------------------------------------------------------------------------------------- //
																MODULE - 3
// ----------------------------------------------------------------------------------------------------------------------------------- //


1. Least-squares method: 
can be defined as a statistical method that is used to find the best-fitting line or plane (in higher dimensions) that minimizes the sum
of the squared differences between the observed and predicted values of the dependent variable. 
	

2. Linear model: 
is an equation that describes a relationship between two quantities that show a constant rate of change. Linear models are widely used in 
fields such as economics, engineering, social sciences, and machine learning. They provide a simple yet powerful way to understand and \
predict relationships between variables.


3. Multiple Regression vs Multi-variate Linear regression:

+ Multiple Regression
	Regression analysis involving multiple independent variables and one dependent variable.
	One dependent variable (response variable) and multiple independent variables (predictor variables).
	y=ßO+ß1x1+ß2x2+ ... + ßn*xn +ε
	Predict the value of the dependent variable based on the values of the independent variables.
	Predicting house prices based on features like size, number of bedrooms, and location.
	Generally less complex compared to multivariate regression.
	
+ Multi-variate Linear regression
	Regression analysis involving multiple dependent variables and two or more independent variables.
	Multiple dependent variables (responses) and two or more independent variables (predictors).
	y1=ß01+ ß11x1+ß21x2+ ... + ßn1xn +ɛ1
	y2=ß02+ß12x1+ß22x2+ ... +ßn2xn + ɛ2
		.
		.
		.
	ym = ß0m + ß1mx1+ß2mx2+ ... +ßnm*xn+ Em
	Understand the relationship between multiple dependent variables and the independent variables.
	Analyzing the effects of factors like humidity, and wind speed on multiple weather variables like temperature, 
	precipitation, etc.
	Generally more complex due to multiple dependent variables and their relationships.


4. Regularization: 
is a crucial technique in machine learning that helps prevent overfitting by adding a penalty term to the model’s objective function 
during training. It helps to control the complexity of the model by discouraging large coefficient values, it discourages the model from 
fitting the training data too closely and promotes simpler models that generalize better to unseen data.
It aims to strike the right balance between bias and variance by controlling the complexity of models. In the context of linear 
regression, regularization typically involves adding a regularization term to the standard least squares cost function. There are two 
common types of regularization techniques used in linear regression: 

	+ Lasso regression: 
		in this, the regularization term added to the cost function is the absolute sum of the coefficients multiplied by a regularization 
		parameter (lambda). It encourages sparsity in the model by driving some coefficient estimates to exactly zero. It automatically 
		selects the most relevant features by setting the coefficients of less important features to zero. This can simplify the model 
		and improve its interpretability. Lasso is effective for feature selection in high-dimensional datasets with many irrelevant 
		features.
			- May not handle multi-collineraity effectively.
			- coefficients can drop to zero abruptly
			- Efficient for higher dimesional datasets

	+ Ridge Regression (L2 Regularization): 
		In Ridge regression, the regularization term added to the cost function is the squared sum of the coefficients multiplied 
		by a regularization parameter (lambda). It helps mitigate overfitting, especially when dealing with multicollinear features.
		The L2 penalty shrinks the coefficients towards zero but does not lead to sparsity. It helps to reduce the variance of the model 
		by reducing the impact of large coefficients, thereby improving its generalization performance.
			- Effective in handling multi-collinearity
			- Coeffiecients are continously shrunk towards zero
			- Computationally expensive for higher dimensional datasets

Importance of regularization in linear regression: 

	+ Prevents Overfitting: 
		Regularization helps prevent overfitting by penalizing overly complex models with large coefficients. By constraining the 
		magnitude of the coefficients, regularization encourages the model to focus on the most important features and reduces its 
		tendency to fit noise in the training data. 
	+ Improves Generalization: 
		Regularization improves the generalization performance of the model by reducing variance. By controlling the complexity of the 
		model, regularization helps to strike a balance between bias and variance, resulting in better performance on unseen data. 
	+ Feature Selection: 
		Lasso regression with L1 regularization encourages sparsity in the coefficients, leading to feature selection. It automatically 
		selects the most relevant features by setting the coefficients of less important features to zero. This can simplify the model 
		and improve its interpretability. 
	+ Stability: 
		Regularization adds stability to the model by reducing the sensitivity of the coefficients to small changes in the training data. 
		This can lead to more consistent and reliable predictions, especially in situations where the training data is limited or noisy.

+ Regularized regression finds application across various domains: 
	 Finance: Predicting stock prices, risk assessment. 
	 Healthcare: Patient outcome prediction, disease diagnosis. 
	 Marketing: Customer churn prediction, sales forecasting. 
	 Environmental Science: Modeling environmental impacts on ecosystems. 


5. Assumptions required for linear regression or multiple regression:
	+ Linearity: 
		The relationship between the independent variables and the dependent variable is linear. This means that changes in the 
		dependent variable are proportional to changes in the independent variables, with constant coefficients. 
	+ Independence of Errors: 
		The errors (residuals) should be independent of each other. In other words, there should be no systematic pattern in the 
		residuals, and the error terms for one observation should not be correlated with the error terms for other observations.
	+ Homoscedasticity: 
		The variance of the errors should be constant across all levels of the independent variables.
	+ Normality of Errors: 
		The errors are normally distributed.
	+ No Multicollinearity: 
		There should be no multicollinearity among the independent variables.
	+ No Autocorrelation: 
		The errors should not be correlated with each other over time or across observations


6. Least-Squares Regression for classification:
also known as Least-Squares Classification (LSC), is a simple method used for binary classification tasks. It aims to find a linear 
decision boundary that separates the classes in the feature space. It seeks to minimize the squared error between the predicted class 
labels and the actual class labels. # (sum of the squared differences between the observed and predicted values of the dependent variable)
Given a dataset with input features and binary class labels (0 or 1), LSC fits a linear regression model to the data.
The model predicts the class labels using a linear equation of the form: 

	+ y = b0 + b1x1 + b2x2 .... bnxn + e
	+ where:
		y = predicted class 0 or 1
		b0, b1, ... bn are the coefficients
		x1 .... xn are the input variables

The decision boundary is determined by the threshold value (e.g., 0.5) applied to the predicted class probabilities.
If the predicted probability is above the threshold, the instance is classified as class 1; otherwise, it is classified as class 0. 

	+ Loss Function: 
		LSC minimizes the squared error loss between the predicted class labels and the actual class labels.
		The loss function penalizes misclassifications by squaring the difference between the predicted and actual class labels. 
		
Least-Squares Regression for classification is a simple and interpretable method commonly used in situations where linear decision 
boundaries are appropriate. It can be applied to various binary classification tasks, such as spam detection, medical diagnosis, and 
sentiment analysis. 


7. Support Vector Machine (SVM): 
is a powerful machine learning algorithm used for various tasks such as classification, regression, and even outlier detection. 
It finds the optimal hyperplane in an N-dimensional space that can separate data points into different classes.
It works by identifying the best separation boundary between classes, maximizing the margin between the classes. 
	(The margin is the distance from the hyperplane to the nearest data point on each side)
SVM chooses the hyperplane that represents the largest separation or margin between the two classes.
	(When such a hyperplane exists, it’s called the maximum-margin hyperplane.)

+ Key Concepts: 

	o Margin: 
		The distance between the hyperplane and the nearest data point from each class. 
		SVM aims to maximize this margin, leading to better generalization. 

	o Kernel function: 
		SVM can handle non-linearly separable data by mapping input features into a higher-dimensional space using 
		kernel functions (e.g., polynomial, radial basis function) to find a linear separation boundary. 

	o Regularization Parameter (C): 
		Controls the trade-off between maximizing the margin and minimizing the classification error on the 
		training data. Higher values of C allow for fewer margin violations but may lead to overfitting. 

	o Hyperplane: is the decision boundary that is used to separate the data points of different classes in a feature space.

	o Support Vectors: are the closest data points to the hyperplane, which makes a critical role in deciding the hyperplane and margin. 

SVM, or Support Vector Machine, can be categorized based on the type of decision boundary they form. Here are the main types:

	+ Linear SVMs: 
		classify data by finding the optimal hyperplane that linearly separates the classes in the feature space. 
		The decision boundary is a straight line (in 2D), or a hyperplane (in higher dimensions) that maximizes the margin between the 
		classes. Linear SVMs are suitable for linearly separable datasets where classes can be separated by a straight line or plane. 

	+ Non-linear SVMs: 
		are used for datasets that are not linearly separable in the original feature space. They employ kernel functions to map the input 
		features into a higher-dimensional space where the classes become separable by a hyperplane. Common kernel functions include 
		polynomial kernel, radial basis function (RBF) kernel, sigmoid kernel, etc. Non-linear SVMs are capable of capturing complex 
		decision boundaries and can handle more intricate patterns in the data.

+ SVM is widely used in various fields, including: 
	o Text classification (e.g., spam detection, sentiment analysis). 
	o Image recognition (e.g., object detection, facial recognition). 
	o Bioinformatics (e.g., gene expression classification, protein structure prediction). 
	o Finance (e.g., credit scoring, stock market prediction). 
	o Medical diagnosis (e.g., disease classification, cancer detection). 
		
+ Comparison with logistic regression:
	The risk of overfitting is generally less in SVM compared to logistic regression.
	SVM works well with unstructured and semi-structured data like text and images.
	SVM can find any arbitrarily shaped decision boundary using the kernel trick, while logistic regression can only separate linearly 
		separable classes.
	SVM finds the “best” margin that separates classes, reducing the risk of error on the data.


// ----------------------------------------------------------------------------------------------------------------------------------- //
																MODULE - 4
// ----------------------------------------------------------------------------------------------------------------------------------- //


1. The Hebbian Learning Rule: 
also known as the Hebb Learning Rule, was proposed by Donald O. Hebb. It’s one of the earliest and simplest learning rules in neural 
networks. Rule states that "neurons that fire together, wire together", it suggests that if two neurons are repeatedly 
activated at the same time, the strength of the connection (synaptic weight) between them should increase.

+ Mechanism: 
	When a presynaptic neuron repeatedly fires and causes the postsynaptic neuron to fire, the connection between them strengthens. 
	If the presynaptic neuron consistently precedes the firing of the postsynaptic neuron, the synaptic connection strengthens further. 
	Conversely, if the presynaptic neuron consistently fails to cause the postsynaptic neuron to fire, the connection weakens.

# Essentially, Hebbian learning allows the network to self-organize by strengthening connections that consistently contribute to the 
# desired output and weakening those that don't.

Hebbian learning is a simple but powerful rule that has been influential in both neuroscience and artificial neural networks. It provides 
a basic building block for understanding how neural networks can learn and adapt. It forms the basis for unsupervised learning algorithms 
such as Hebbian learning and competitive learning, where networks self organize based on input patterns. 


2. The Expectation-Maximization (EM) algorithm: 
is a powerful iterative optimization method commonly used in machine learning and statistics. It’s particularly useful for estimating 
parameters in probabilistic models where some variables are hidden or unobserved. 

@ In real-world machine learning applications, we often encounter situations where some features are observable while others remain hidden 
or partially observable.

@ The EM algorithm addresses this by leveraging the instances where the variable is observable to learn and predict its values in instances 
where it’s not observable. 

@ It’s especially applicable to latent variables (variables inferred from other observed variables but not directly observable).

EM works in an iterative fashion, consisting of two main steps:

	+ Expectation Step (E-step): 
		In this step, the algorithm estimates the probability of each data point belonging to each cluster, given the current model 
		parameters. It essentially uses the current cluster assignments to calculate 
		the likelihood of data points being in those clusters.

	+ Maximization Step (M-step): 
		Here, the algorithm uses the probabilities estimated in the E-step to refine the model parameters. It aims to find the parameters 
		that maximize the overall likelihood of the data given the current cluster assignments. This improves 
		the model's ability to represent the data's clustering structure.

	+ Convergence:
		The EM algorithm iterates between the E-step and M-step until convergence i.e likelihood of the data (given the model parameters) 
		stops increasing significantly. At this point, the EM algorithm has identified a maximum likelihood solution for the model 
		parameters and the latent cluster assignments for each data point.

@ Advantages:
	-The EM algorithm for clustering is widely used in various domains, including image segmentation, document clustering, and gene 
		expression analysis. 
	-It is particularly useful when the data contains hidden or latent variables and when the underlying data distribution is complex 
		and cannot be easily modeled by a single probability distribution.


// ----------------------------------------------------------------------------------------------------------------------------------- //
																MODULE - 5
// ----------------------------------------------------------------------------------------------------------------------------------- //


1. Introduction,  Fundamental concept,  Evolution of Neural Networks:  
Neural Networks are computational models that mimic the complex functions of the human brain. The neural networks consist of 
interconnected nodes or neurons that process and learn from data, enabling tasks such as pattern recognition and decision making in 
machine learning. 
	-1940s 1st neuron but did'nt worked due to lack of computaional capabilities
	-1960s perceptron
	-then came backproposition and connectionism
	-then early 2000s deep learning was Introduced
	-Convolutional neural networks (CNNs) and recurrent neural networks (RNNs), two deep learning architectures, dominated machine 
		learning.


2. Biological Neuron,  Artificial Neural Networks,  NN architecture: 

	- BNNs: 
		refers to a system of interconnected brain cells, known as neurons, that make up the central nervous system in living organisms

	- Neurons: 
		These are the basic building blocks of BNNs. Neurons have multiple dendrites that receive input signals from other 
		neurons. The soma (cell body) processes these incoming signals, and the axon transmits the processed signals to other cells.

	- Synapses: 
		These are the points of connection between neurons, where information is transmitted. 
		# The contact point between the axon terminal of one neuron and the dendrite of another neuron.
		In BNNs, synapses are flexible and can be modified by learning and experience.

	+ Advantages of BNN:   
		 The synapses are the input processing element. 
		 It is able to process highly complex parallel inputs. 

	+ Disadvantages of BNN: 	
		 There is no controlling mechanism. 
		 Speed of processing is slow being it is complex. 

	- ANNs: 
		are mathematical models inspired by the organization of biological neural networks. Interconnected 
		artificial neurons are arranged in a series of layers that together constitute the whole Artificial Neural Network in a 
		system. A layer can have dozen of units or millions of units as this depends on how the complex neural networks will be 
		required to learn the hidden patterns in the dataset. Commonly, Artificial Neural Network has an input layer, an output layer 
		as well as hidden layers. 
			-The input layer receives data from the outside world which the neural network needs to analyze or learn about. 
			-Then this data passes through one or multiple hidden layers that transform the input into data that is valuable for the 
			output layer. 
				+Each neuron in a layer connects to every neuron in the next layer through weighted connections.
				+Each connection has a weight, and neurons apply an activation function to the weighted sum of inputs to decide the output
			-Finally, the output layer provides an output in the form of a response of the Artificial Neural Networks to input 
			data provided. Each neuron in this layer represents a possible output.
		
Neural Network Architecture: There are different types of neural network architectures:

	- Feedforward Neural Networks (FNNs): 
		These are the simplest type of ANNs. Information flows from input nodes to output nodes without cycles.

	- Convolutional Neural Networks (CNNs): 
		Designed for image and video recognition. They use convolutional layers to extract features.

	- Recurrent Neural Networks (RNNs): 
		Suitable for sequential data (e.g., time series). They have loops to allow information to persist.


5. McCulloch-Pitts Model. Designing a simple network: 
is a simplified neural model that represents the behavior of a biological neuron, it was the earliest ANN model.  
In this model a neuron is represented as a binary threshold unit.
Each neuron receives input signals from other neurons or external sources.
The inputs are either excitatory (positive) or inhibitory (negative)
The excitatory inputs have weights of positive magnitude and the inhibitory weights have weights of negative magnitude. 
The inputs of the McCulloch-Pitts neuron could be either 0 or 1. 
It has a threshold function also called as activation function which computes a weighted sum of its inputs.
If the sum exceeds a predefined threshold, the neuron fires (outputs 1); otherwise, it remains inactive.

Input Signals: These are the neuron's inputs, represented as x1, x2, x3. Each input has a corresponding weight.
Weights: Each input signal is associated with a weight w1, w2, w3 representing its importance. These weights are real numbers

	+ weighted sum = summation of wi * xi
	
	+ Applications:
		The McCulloch-Pitts model laid the foundation for artificial neural networks.
		Although it’s a simplified model, it helped researchers understand the basic principles of neural computation.


6. Perceptron model with Bias: 
The perceptron model, an advancement over the MP neuron model, introduces a bias term to enhance its capabilities, it was originally 
proposed by Frank Rosenblatt in 1957. It consists of a single layer of neurons, each connected to the inputs via weighted connections. 
Here's an explanation of the perceptron model with bias: 

	+ Inputs: 
		The perceptron model takes a set of input values x1,x2,...,xn, each representing a feature or attribute of the input data. 

	+ Weights: 
		Each input is associated with a weight w1,w2,...,wn, which determines the importance of that input in the computation. 
		These weights are adjusted during the training process to optimize the performance of the perceptron. 

	+ Bias: 
		The bias term, denoted as b, is an additional parameter in the perceptron model. It helps shift the decision boundary 
		It allows the Perceptron to learn an offset from the origin. 

	+ Weighted Summation: 
		Like the MP neuron model, the inputs are multiplied by their corresponding weights and summed up. Additionally, the bias term is 
		added to the weighted sum: summation of wi * xi + bias

	+ Activation Function: 
		In the basic perceptron model, the activation function is a step function, which outputs 1 if the weighted sum  of inputs plus 
		bias exceeds a certain threshold, and 0 otherwise. Mathematically, the output of the perceptron can be represented as: summation 
		of wi * xi + bias > 0

	+ Learning: 
		The perceptron learns by adjusting its weights and bias based on the observed errors. It uses a learning algorithm, such as the 
		perceptron learning rule, to update the weights and bias iteratively until it achieves the desired output for a given set of 
		inputs

	+ Limitations:
		Limited to linearly separable problems; unable to learn non-linear decision boundaries.
		Convergence is not guaranteed if the data is not linearly separable

7. Activation functions,  Binary,  Bipolar,  continuous,  Ramp:

	+ Activation functions: 
		are mathematical functions used in artificial neural networks to introduce non-linearity into the system, allowing 
		the network to learn and model complex relationships in the data.

	+ Binary Activation Function:- Output is binary: 1 if input is greater than or equal to zero, 0 otherwise.
	+ Bipolar Activation Function:- Output is bipolar: 1 if input is positive,-1 if input is negative
	+ Continuous Activation Function: 
		Provides continuous output across the entire input range.
		Examples include sigmoid, tanh, and softmax functions
			. Sigmoid function: f(x) = (1 / 1 + e^-x) squashes the input values between 0 and 1.
			. Tanh function: f(x) = (e^x - e^-x / e^x + e^-x) squashes the input values between -1 and 1.
			. Softmax function: f (xi) = (e^xi / summation e^xj) used in multi-class classification to output probabilities for each class
	+ Ramp Activation Function:
		Gradually increases output as input increases, instead of abruptly changing at a threshold.


8. Delta Learning Rule / The LMS (Least Mean Square) algorithm: 
also known as the Widrow-Hoff learning rule, is a fundamental algorithm in machine learning and adaptive signal processing. Developed by 
Bernard Widrow and Ted Hoff, it works by iteratively adjusting weights in a linear model to minimize the difference between 
the desired output and the actual output produced. The goal is to Minimize the mean squared error between the predicted and desired output.
It is used in Supervised learning tasks where a training dataset with known inputs and desired outputs is available.

Working:
	+ Initialization: 
		Initialize weights randomly.
	+ Iterative Update:
		Weights are updated based on following condition:
			w(t+1)= w(t) + η(d(t)−y(t)) x (t)
		+ Here,
			𝑤(𝑡) & w(t+1) are the weight vectors before and after the update, respectively.
			η is the learning rate, a small positive constant that determines the step size of the weight update.
			d(t) is the desired output at time t.
			y(t)is the predicted output at time t.
			x(t) is the input vector at time t.
	+ Convergence: 
		This process is repeated multiple times until the error converges to an acceptable minimum value.
	
Applications:
	The Widrow-Hoff algorithm finds applications in various fields, including adaptive filtering, noise cancellation, Pattern Recognition,
	Signal Processing, and linear regression problems


9. Multi-layer perceptron network: 
A basic perceptron works well for data sets which possess linearly separable patterns. 
However, in practical situations, that is not an ideal situation to have.  
A multi-layer perceptron (MLP) is a type of artificial neural network (ANN) consisting of multiple layers of interconnected neurons. 
The major highlights of this model are as follows: 
	 The neural network contains one or more intermediate layers between the input and output nodes, which are hidden from both input and 
	output nodes 
	 Each neuron in the network includes a non-linear activation function that is differentiable. 
	 The neurons in each layer are connected with some or all the neurons in the previous layer. 
	 They can learn nonlinear relationships in data, making them suitable for real-world problems.
	 MLPs are powerful models for tasks such as classification, regression, and pattern recognition.


10. Error Backpropagation: 
is a fundamental algorithm used to train artificial neural networks, especially those with a feed-forward architecture. It's essentially 
a way for the network to learn by iteratively adjusting its internal connections to minimize the error between its outputs and the 
desired targets. How it works:

	+ Forward Pass: 
		The network receives an input, and this input is propagated forward through the network layer by layer. At each layer, the 
		weighted sum of the previous layer's outputs is calculated, and an activation function is applied to determine the output of 
		the current layer.

	+ Error Calculation: 
		Once the input reaches the output layer, the errors between the network's predictions and the desired targets are calculated.
		This error is often quantified using a cost function, which measures the overall performance of the network.
	
	+ Backward Pass (Backpropagation): 
		The errors are propagated backward through the network, starting from the output layer and 
		going all the way back to the input layer. This is why it's called backpropagation. During backpropagation, the algorithm 
		calculates how much each weight contributed to the error. This is done using partial derivatives.
		The general formula for the weight update in backpropagation is:
			Δw = - η * ∂E / ∂w		
		- where:
			Δw (delta w) is the update value for the weight
			η (eta) is the learning rate, which controls the step size of the weight update
			∂E / ∂w is the partial derivative of the cost function with respect to the weight

	+ Iteration: 
		Steps 1 to 3 are repeated for multiple training examples (epochs). With each iteration, the network learns and adjusts its 
		internal connections, getting better at mapping inputs to the desired outputs.

By continuously adjusting the connections based on the error, backpropagation allows the neural network to learn complex patterns from 
data. This is what makes it a powerful tool for various machine learning applications.


// ----------------------------------------------------------------------------------------------------------------------------------- //
																MODULE - 6
// ----------------------------------------------------------------------------------------------------------------------------------- //


1. Curse of Dimensionality:  
refers to the various challenges and limitations that arise when dealing with high-dimensional data spaces. As the number of dimensions (features or variables) increases, several issues emerge that can significantly impact the 
effectiveness of various algorithms and techniques used in machine learning and data analysis. Here are some key 
aspects of the Curse of Dimensionality: 

	+ Increased Sparsity: 
		As the dimensionality increases, the available data becomes sparser. 
		In high-dimensional spaces, the data points tend to spread out, leading to 
		fewer data points per unit volume. This sparsity can make it difficult to generalize 
		from the data or to estimate reliable statistics. 

	+ Increased Computational Complexity: 
		Algorithms often become computationally more demanding as the dimensionality increases. 
		Many algorithms have a time complexity that grows exponentially with the number of dimensions. 
		This makes tasks such as distance calculations, clustering, and optimization more computationally expensive and time-consuming. 

	+ Degraded Performance of Distance-based Methods: 
		Distance-based methods, such as k-nearest neighbours (k-NN) and clustering algorithms, rely on measuring distances 
		between data points. Due to the data being spread out sparsly such methods are no longer suitable.

	+ Overfitting: 
		With an increasing number of dimensions, the risk of overfitting also increases. 
		In high-dimensional spaces, there is a higher likelihood that models will 
		capture noise in the data, leading to poor generalization performance on unseen data. 


12. Dimensionality Reduction Techniques:  
Dimensionality reduction is the process of reducing the number of features in a 
dataset while retaining as much information as possible. This can be done for a variety of reasons, such as to reduce the 
complexity of a model, to improve the performance of a learning algorithm, or to make it easier to visualize the data. 
Some common dimensionality reduction techniques include:

	- PCA: statistical technique which transforma the original features into a set of orthogonal axes that capture maximum variance 
	in the data.

	- Linear Discriminant Analysis (LDA): is a supervised dimensionality reduction technique that finds the linear combinations 
	of features that best separate different classes in the data. It aims to maximize the between-class variance while 
	minimizing the within-class variance.

	- t-Distributed Stochastic Neighbour Embedding (t-SNE): t-SNE is a technique commonly used for visualizing high-dimensional 
	data by reducing it to two or three  dimensions.

	- Isomap: Isomap is a nonlinear dimensionality reduction technique that constructs a low-dimensional embedding of the data 
	based on the geodesic distances in a high-dimensional space. 


13. Principal Component Analysis:  
is a widely used technique in statistics and data science for dimensionality reduction.  
Its primary goal is to simplify complex data sets by transforming them into a new coordinate system where set of orthogonal 
axes capture the maximum varience in the data. These are called principal components and are a linear combinations of the original 
variables in the dataset. Here's  a  breakdown  of  how  PCA  works: 

	+ Data Centering: 
		The first step in PCA involves centering the data by subtracting the mean of each feature from the respective feature values. 
		This ensures that the new coordinate system is not biased towards any particular feature. 

	+ Covariance Matrix Computation: 
		After centering the data, PCA calculates the covariance matrix, which summarizes the relationships between different features. 
		The covariance between two features measures how they vary together. 
		A positive covariance indicates that they tend to increase or decrease together, while a negative covariance indicates an 
		inverse relationship. 

	+ Eigenvalue Decomposition: 
		The covariance matrix is then decomposed into its eigenvectors and eigenvalues. 

	+ Selection of Principal Components: 
		The eigenvectors are ranked based on their corresponding eigenvalues. 
		The eigenvector with the highest eigenvalue represents the direction of maximum variance in the data and is termed the first 
		principal component (PC). 
		Subsequent eigenvectors represent directions of decreasing variance and are termed the second, third, and so on, principal 
		components. 

	+ Projection: 
		Finally, PCA projects the original data onto the new coordinate system defined by the principal components. 


































1) SVD: It stands for singular value decomposition, it is a matrix factorization technique. SVD is a method for decomposing a matrix. For a given matrix "A"
it can be decomposed into three matrices "U", "∑" & "V^T" such that: 
	
	- A = U ∑ V^T

Given a matrix "A" of "m" * "n" dimesnsion: 
	- "U" is the m x m orthogonal matrix called left singular matrix.
	- "∑" Is m x n diagonal matrix 
	- "V^T" is n x n orthogonal matrix called right singular matrix.

Applications:
	- Pseudo-inverse Calculation
	- Homogeneous Linear Equations
	- To find Rank, Range, and Null Space of a matrix
	- Matrix apporximation

Practical Use Cases:
	- Data Compression: SVD helps find the best low-rank approximation to a matrix, reducing its dimensionality.
	- Signal Processing: SVD aids in noise reduction, feature extraction, and denoising.
	- Process Control: SVD assists in modeling and analyzing complex systems.


# 6. Least Squares Regression for classification: (logistic regression)
# commonly known as Logistic Regression, is a supervised learning algorithm used for binary classification tasks. In binary classification, 
# the target variable has two possible outcomes, typically represented as 0 and 1.  It predicts the output of categorical dependent variable, 
# based on multiple input variables.
# In Logistic Regression, the goal is to estimate the probability that a given input belongs to a particular class. The algorithm models 
# this probability using the logistic function (sigmoid function) also know as Sigmoid function it is a mathematical function used to map 
# the predicted values to probabilities. It maps any real value into another value within a range of 0 and 1.
# Mathematically, the logistic function is defined as:
# 	+ f(x) = 1 / 1 + e^-x
# During training, the model's parameters (weights and bias) are optimized to maximize the likelihood of the observed data. This is 
# typically done using optimization algorithms such as gradient descent. Once trained, the logistic regression model can predict
# the	probability that an instance belongs to a given class. If the predicted probability is greater than a threshold value 
# (usually 0.5), the instance is classified into Class 1; otherwise, it belongs to Class 0.	
# Logistic Regression is widely used due to its simplicity, interpretability, and efficiency, especially for linearly separable datasets. 
# However, it's important to note that it assumes a linear relationship between the input features and the log-odds of the output, which 
# may not always hold true in practice.










